{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "># 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature engineering 편의성을 위해 test셋을 train 폴더에 복사해서 전처리를 한번에 하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = pd.read_csv(\"./train/ace_2000.csv\")\n",
    "tmp_data.head(20)\n",
    "del tmp_data\n",
    "## 12번 row까지는 생략이 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train\\ace_1999.csv 의 데이터 길이 : 492764\n",
      "./train\\ace_2000.csv 의 데이터 길이 : 494114\n",
      "./train\\ace_2001.csv 의 데이터 길이 : 492766\n",
      "./train\\ace_2002.csv 의 데이터 길이 : 492764\n",
      "./train\\ace_2003.csv 의 데이터 길이 : 492763\n",
      "./train\\ace_2004.csv 의 데이터 길이 : 494113\n",
      "./train\\ace_2005.csv 의 데이터 길이 : 492764\n",
      "./train\\ace_2006.csv 의 데이터 길이 : 492764\n",
      "./train\\ace_2007.csv 의 데이터 길이 : 492764\n",
      "./train\\ace_2008.csv 의 데이터 길이 : 494112\n",
      "./train\\ace_2009.csv 의 데이터 길이 : 492759\n",
      "./train\\ace_2010.csv 의 데이터 길이 : 525613\n",
      "./train\\ace_2011.csv 의 데이터 길이 : 492762\n",
      "./train\\ace_2012.csv 의 데이터 길이 : 494113\n",
      "./train\\ace_2013.csv 의 데이터 길이 : 492764\n"
     ]
    }
   ],
   "source": [
    "##데이터 길이 확인\n",
    "file_name = glob.glob(\"./train/ace_\"+\"*.csv\")\n",
    "for file in file_name:\n",
    "    print(file+' 의 데이터 길이 : '+str(len(pd.read_csv(file))))\n",
    "    \n",
    "del file_name\n",
    "## 결과로 결측값이 존재하는 것을 확인함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 데이터 csv 형식으로 변경 완료\n",
      "2000 데이터 csv 형식으로 변경 완료\n",
      "2001 데이터 csv 형식으로 변경 완료\n",
      "2002 데이터 csv 형식으로 변경 완료\n",
      "2003 데이터 csv 형식으로 변경 완료\n",
      "2004 데이터 csv 형식으로 변경 완료\n",
      "2005 데이터 csv 형식으로 변경 완료\n",
      "2006 데이터 csv 형식으로 변경 완료\n",
      "2007 데이터 csv 형식으로 변경 완료\n",
      "2008 데이터 csv 형식으로 변경 완료\n",
      "2009 데이터 csv 형식으로 변경 완료\n",
      "2010 데이터 csv 형식으로 변경 완료\n",
      "2011 데이터 csv 형식으로 변경 완료\n",
      "2012 데이터 csv 형식으로 변경 완료\n",
      "2013 데이터 csv 형식으로 변경 완료\n"
     ]
    }
   ],
   "source": [
    "all_data = list()\n",
    "for year in range(1999,2014) :\n",
    "    print(year, '데이터 csv 형식으로 변경 완료')\n",
    "    tmp_df = pd.read_csv(\"./train/ace_\"+str(year)+\".csv\")\n",
    "    tmp_df = tmp_df[11:]\n",
    "    col_names = list(map(lambda x: x.split(), list(tmp_df[:1].values[0])))[0]\n",
    "    tmp_df = tmp_df[2:]\n",
    "    splited_rows = list(map(lambda x: x.split(),tmp_df[tmp_df.columns[0]][2:]))\n",
    "    new_df = pd.DataFrame(splited_rows,columns=col_names)\n",
    "    new_df['date'] = new_df['doy'].apply(lambda x : datetime.date(int(new_df.year[0])-1,12,31) + datetime.timedelta(days = int(x)))\n",
    "    new_df['hr'] = new_df['hr'].apply( lambda x : str(x).zfill(2) )\n",
    "    new_df['min'] = new_df['min'].apply( lambda x : str(x).zfill(2) )\n",
    "    new_df['date'] = new_df['date'].apply(lambda x : str(x))\n",
    "    new_df['date'] += ' ' + new_df['hr']\n",
    "    new_df['date'] += ':' + new_df['min']\n",
    "    all_data.append(new_df)\n",
    "\n",
    "all_df = pd.concat(all_data)\n",
    "# del all_data, new_df, splited_rows, tmp_df, col_names\n",
    "\n",
    "# all_df.to_csv(\"./train/train_new.csv\",  index=False)\n",
    "# train_data = pd.read_csv(\"./train/train_x.csv\")\n",
    "## 저장하고 다시 불러오면 object가 float로 변하는 등 원래 데이터 타입을 찾아감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_data, new_df, splited_rows, tmp_df, col_names\n",
    "\n",
    "all_df.to_csv(\"./train/train_new.csv\",  index=False)\n",
    "train_data = pd.read_csv(\"./train/train_new.csv\")\n",
    "# 저장하고 다시 불러오면 object가 float로 변하는 등 원래 데이터 타입을 찾아감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9999.9    -9999.9004]\n",
      "[-9999.9    -9999.9004]\n",
      "[-9999.9    -9999.9004]\n",
      "[-9999.9    -9999.9004]\n",
      "[-9999.9    -9999.9004]\n",
      "[-9999.9    -9999.9004]\n",
      "[-9999.9    -9999.9004]\n"
     ]
    }
   ],
   "source": [
    "for col in list(train_data.columns)[4:-1]:\n",
    "    print(train_data[col][train_data[col] <= -9999].unique())\n",
    "### 이를 통해 NaN 값은 -9999.9와 -9999.9004가 존재하는 것을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace(-9999.9 , np.nan)\n",
    "train_data = train_data.replace(-9999.9004 , np.nan)\n",
    "## np.nan으로 변경\n",
    "\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "## data market과 join하기 위해 datetime 형식으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 data market 생성완료\n",
      "2000 data market 생성완료\n",
      "2001 data market 생성완료\n",
      "2002 data market 생성완료\n",
      "2003 data market 생성완료\n",
      "2004 data market 생성완료\n",
      "2005 data market 생성완료\n",
      "2006 data market 생성완료\n",
      "2007 data market 생성완료\n",
      "2008 data market 생성완료\n",
      "2009 data market 생성완료\n",
      "2010 data market 생성완료\n",
      "2011 data market 생성완료\n",
      "2012 data market 생성완료\n",
      "2013 data market 생성완료\n"
     ]
    }
   ],
   "source": [
    "## data market을 생성하고 기존 train data를 merge\n",
    "## train data 확인 결과 1999~2012년 까지 데이터가 존재하는 것으로 확인됨\n",
    "\n",
    "train_market = list()\n",
    "for year in range(1999, 2014):\n",
    "    tmp_date = pd.date_range(start='01/01/'+str(year), end='12/31/'+str(year))\n",
    "    tmp_df_date = pd.DataFrame(tmp_date, columns=['date'])\n",
    "    tmp_df_date['date'] = tmp_df_date['date'].astype(str)\n",
    "    tmp_df_date['date'] = tmp_df_date['date'].str[0:10]\n",
    "    \n",
    "    tmp_time = pd.date_range(start='00:00:00', end='23:59:00', freq='1min')\n",
    "    tmp_df_time = pd.DataFrame(tmp_time, columns=['time'])\n",
    "    tmp_df_time['time'] = tmp_df_time['time'].astype(str)\n",
    "    tmp_df_time['time'] = tmp_df_time['time'].str[10:19]\n",
    "    \n",
    "    tmp_init_DT = pd.DataFrame(columns=['date', 'time'])\n",
    "    tmp_init_DT2 = pd.DataFrame(columns=['date', 'time'])\n",
    "    \n",
    "    for i in range(len(tmp_date)):\n",
    "        if i == 0:\n",
    "            tmp_init_DT['time'] = tmp_df_time['time']\n",
    "            tmp_init_DT['date'] = tmp_df_date['date'][0]\n",
    "        else:\n",
    "            tmp_init_DT2['time'] = tmp_df_time['time']\n",
    "            tmp_init_DT2['date'] = tmp_df_date['date'][i]\n",
    "            tmp_init_DT = tmp_init_DT.append(tmp_init_DT2)\n",
    "    \n",
    "    train_market.append(tmp_init_DT)\n",
    "    print('%d data market 생성완료' %year)\n",
    "\n",
    "del tmp_df_time, tmp_df_date, tmp_init_DT2, tmp_init_DT\n",
    "\n",
    "train_market = pd.concat(train_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Np         0\n",
       "Tp         0\n",
       "Vp         0\n",
       "B_gsm_x    0\n",
       "B_gsm_y    0\n",
       "B_gsm_z    0\n",
       "Bt         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_market['time_conv'] = train_market['date'] + train_market['time']\n",
    "train_market = train_market.drop(columns=['date', 'time'])\n",
    "\n",
    "train_data = train_data.rename(columns={'date':'time_conv'})\n",
    "## join을 위해 변경\n",
    "\n",
    "train_market['time_conv'] = pd.to_datetime(train_market['time_conv'])\n",
    "## join을 위해 변경\n",
    "\n",
    "join_result = pd.merge(train_market, train_data, how = 'left', on = 'time_conv')\n",
    "## 1분 단위로 모든 시간이 있는 data market에 join하여 nan값 전처리 준비\n",
    "\n",
    "join_result.to_csv(\"./train/train_1min.csv\",  index=False)\n",
    "join_result = pd.read_csv(\"./train/train_1min.csv\")\n",
    "## 저장 후 불러와서 자료형 일치 시킴\n",
    "\n",
    "join_result = join_result.drop(columns=['year', 'doy', 'hr', 'min'])\n",
    "join_result['time_conv'] = pd.to_datetime(join_result['time_conv'])\n",
    "tmp_data_10min = join_result.set_index('time_conv')\n",
    "tmp_data_10min = tmp_data_10min.resample('10Min').mean()\n",
    "\n",
    "del join_result\n",
    "\n",
    "## 10분 단위 평균으로 데이터셋 변경\n",
    "\n",
    "tmp_data_10min = tmp_data_10min.interpolate(method='linear')\n",
    "tmp_data_10min.isnull().sum()\n",
    "## 선형 interpolation으로 nan 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3시간 단위로 resampling\n",
    "\n",
    "tmp_data_3H_mean = tmp_data_10min.resample('3H').mean()\n",
    "tmp_data_3H_max = tmp_data_10min.resample('3H').max()\n",
    "tmp_data_3H_min = tmp_data_10min.resample('3H').min()\n",
    "tmp_data_3H_std = tmp_data_10min.resample('3H').std()\n",
    "tmp_data_3H_var = tmp_data_10min.resample('3H').var()\n",
    "\n",
    "train_data_3H_col = list()\n",
    "for str in ['mean_', 'max_', 'min_', 'std_', 'var_']:\n",
    "    train_data_3H_col.append(list(map(lambda x: str+x, list(tmp_data_10min.columns))))\n",
    "\n",
    "tmp_data_3H_mean.columns = train_data_3H_col[0]\n",
    "tmp_data_3H_max.columns = train_data_3H_col[1]\n",
    "tmp_data_3H_min.columns = train_data_3H_col[2]\n",
    "tmp_data_3H_std.columns = train_data_3H_col[3]\n",
    "tmp_data_3H_var.columns = train_data_3H_col[4]\n",
    "\n",
    "tmp_data_3H_mean.to_csv('tmp_data_3H_mean.csv',index=False)\n",
    "tmp_data_3H_max.to_csv('tmp_data_3H_max.csv',index=False)\n",
    "tmp_data_3H_min.to_csv('tmp_data_3H_min.csv',index=False)\n",
    "tmp_data_3H_std.to_csv('tmp_data_3H_std.csv',index=False)\n",
    "tmp_data_3H_var.to_csv('tmp_data_3H_var.csv',index=False)\n",
    "\n",
    "tmp_train0 = pd.DataFrame(tmp_data_3H_mean.index, columns=['time_conv'])\n",
    "tmp_train1 = pd.read_csv('tmp_data_3H_mean.csv')\n",
    "tmp_train2 = pd.read_csv('tmp_data_3H_max.csv')\n",
    "tmp_train3 = pd.read_csv('tmp_data_3H_min.csv')\n",
    "tmp_train4 = pd.read_csv('tmp_data_3H_std.csv')\n",
    "tmp_train5 = pd.read_csv('tmp_data_3H_var.csv')\n",
    "\n",
    "train_fin = pd.concat([tmp_train0, tmp_train1, tmp_train2, tmp_train3, tmp_train4, tmp_train5], axis=1)\n",
    "del tmp_train0, tmp_train1, tmp_train2, tmp_train3, tmp_train4, tmp_train5\n",
    "\n",
    "tmp_test = pd.read_csv(\"Kp_index.csv\")\n",
    "kp_list = list()\n",
    "for i in range(len(tmp_test.values)):\n",
    "    kp_list += list(tmp_test.values[i][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin.to_csv('train_fin.csv',index=False)\n",
    "train_fin = pd.read_csv('train_fin.csv')\n",
    "\n",
    "tmp_df = pd.DataFrame(train_fin['time_conv'].str.split(' ').tolist(),columns = ['date','time'])\n",
    "tmp_df2 = pd.DataFrame(tmp_df['date'].str.split('-').tolist(),columns = ['year','month', 'day'])\n",
    "tmp_df3 = pd.DataFrame(tmp_df['time'].str.split(':').tolist(),columns = ['hour','min', 'sec'])\n",
    "tmp_df3 = tmp_df3.drop(columns=['sec','min'])\n",
    "train_fin = train_fin.drop(columns=['time_conv'])\n",
    "tmp_df4= pd.concat([tmp_df2, tmp_df3], axis=1)\n",
    "train_fin = pd.concat([tmp_df4, train_fin], axis=1)\n",
    "del tmp_df, tmp_df2, tmp_df3, tmp_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fin = train_fin[train_fin['year'] == '2013']\n",
    "train_fin = train_fin[train_fin['year'] != '2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin['target'] = kp_list\n",
    "del kp_list, tmp_test\n",
    "\n",
    "## 예측해야하는 목표값인 kp index 파일을 3시간 단위로 만든 train데이터에 붙이는 작업 실시\n",
    "## train data에 kp를 붙여서 예측할 target 까지 포함한 데이터셋으로 제작\n",
    "\n",
    "train_fin.to_csv('train_fin.csv',index=False)\n",
    "test_fin.to_csv('test_fin.csv',index=False)\n",
    "del train_fin, test_fin\n",
    "###완성파일 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 2. 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fin = pd.read_csv('train_fin.csv')\n",
    "test_fin = pd.read_csv('test_fin.csv')\n",
    "## target을 shift해서 현재의 feature로 다음 3시간을 예측할 수 있도록 설정\n",
    "## 위에 순서까지 진행을 했으면 파일이 보존되어 있으므로 여기부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test_true : 실제 계측 값(list)\n",
    "prediction_value : 모델이 예측한 값(list)\n",
    "'''\n",
    "## WRMSE 정의\n",
    "\n",
    "def WRMSE(test_data, prediction_data):\n",
    "    \n",
    "    ## weight 계산\n",
    "    unique_list = list(np.unique(test_data))\n",
    "    weight_list = list()\n",
    "    for count_number in unique_list:\n",
    "        weight_list.append(test_data.count(count_number))\n",
    "    \n",
    "    ## weighted RMSE 계산\n",
    "    wrmse_list = []\n",
    "    temp_summation = 0\n",
    "    for element in enumerate(prediction_data):\n",
    "        temp_summation += (1/weight_list[unique_list.index(test_data[element[0]])])*pow(test_data[element[0]] - element[1], 2)\n",
    "    wrmse_result = math.sqrt(temp_summation)\n",
    "    \n",
    "    return wrmse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=None, n_splits=12)\n"
     ]
    }
   ],
   "source": [
    "index_list = train_fin.columns.tolist()\n",
    "index_list.pop(index_list.index('target'))\n",
    "feature_label = index_list\n",
    "del index_list\n",
    "target_label = ['target']\n",
    "\n",
    "feature_data = train_fin[feature_label].values\n",
    "target_data = train_fin[target_label].values\n",
    "tscv = TimeSeriesSplit(max_train_size=None, n_splits=12)\n",
    "print(tscv)\n",
    "\n",
    "## Time Series Split 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.168692749442476\n",
      "4.745529275684246\n",
      "3.68442006245481\n",
      "4.500364482634532\n",
      "4.362461868887428\n",
      "4.5704211761938645\n",
      "3.9342959690375565\n",
      "2.813375257550505\n",
      "5.027560599003812\n",
      "3.29784048247418\n",
      "4.217434005883557\n",
      "3.2497520550633245\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(feature_data):\n",
    "    X_train, X_test = feature_data[train_index], feature_data[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    rfr = RandomForestRegressor()\n",
    "    rfr.fit(X_train, y_train)\n",
    "    y_preds = rfr.predict(X_test)\n",
    "\n",
    "    WRMSE_true = list(map(lambda x : x[0], list(y_test)))\n",
    "    y_preds = list(np.around(np.array(list(y_preds)),0))\n",
    "    y_preds = list(map(lambda x : 0 if x <= 0 else x, y_preds))\n",
    "    WRMSE_preds = list(map(lambda x : 9 if x >= 9 else x, y_preds))\n",
    "    print(WRMSE(WRMSE_true, WRMSE_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.192075176348891\n",
      "4.239603519010367\n",
      "3.4944893636335967\n",
      "4.30994003120945\n",
      "4.004120416580994\n",
      "4.170375802476961\n",
      "4.214024299132223\n",
      "2.8549078905759866\n",
      "4.8187138339488405\n",
      "3.537550336390857\n",
      "4.314361079543012\n",
      "3.4209401558502868\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(feature_data):\n",
    "    X_train, X_test = feature_data[train_index], feature_data[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    \n",
    "    WRMSE_true = list(map(lambda x : x[0], list(y_test)))\n",
    "    y_preds = list(np.around(np.array(list(y_preds)),0))\n",
    "    y_preds = list(map(lambda x : 0 if x <= 0 else x, y_preds))\n",
    "    WRMSE_preds = list(map(lambda x : 9 if x >= 9 else x, y_preds))\n",
    "    print(WRMSE(WRMSE_true, WRMSE_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9779131153449505\n",
      "4.3363675019371435\n",
      "3.50841950144898\n",
      "4.32609750940869\n",
      "3.8291777945802434\n",
      "3.9955059545544063\n",
      "3.931601252510613\n",
      "2.7698374576161293\n",
      "4.886394023979587\n",
      "3.4914335591048204\n",
      "4.15470467303347\n",
      "3.1864587514127667\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(feature_data):\n",
    "    X_train, X_test = feature_data[train_index], feature_data[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror', booster='gbtree', learning_rate=0.2, min_child_weight=1.5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    \n",
    "    WRMSE_true = list(map(lambda x : x[0], list(y_test)))\n",
    "    y_preds = list(np.around(np.array(list(y_preds)),0))\n",
    "    y_preds = list(map(lambda x : 0 if x <= 0 else x, y_preds))\n",
    "    WRMSE_preds = list(map(lambda x : 9 if x >= 9 else x, y_preds))\n",
    "    print(WRMSE(WRMSE_true, WRMSE_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.612793047775386\n",
      "5.240436826058216\n",
      "3.5734414344850562\n",
      "5.166631745367273\n",
      "4.417699022668659\n",
      "4.457813302807302\n",
      "3.9565111646756983\n",
      "2.73732797242002\n",
      "4.812720078411192\n",
      "3.6508338420720516\n",
      "3.7530757802515504\n",
      "3.0917429645752765\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(feature_data):\n",
    "    X_train, X_test = feature_data[train_index], feature_data[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror', booster='gbtree', learning_rate=0.15, min_child_weight=0.5,\n",
    "                            max_depth=7)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    \n",
    "    WRMSE_true = list(map(lambda x : x[0], list(y_test)))\n",
    "    y_preds = list(np.around(np.array(list(y_preds)),0))\n",
    "    y_preds = list(map(lambda x : 0 if x <= 0 else x, y_preds))\n",
    "    WRMSE_preds = list(map(lambda x : 9 if x >= 9 else x, y_preds))\n",
    "    print(WRMSE(WRMSE_true, WRMSE_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(max_train_size=None, n_splits=12)\n"
     ]
    }
   ],
   "source": [
    "index_list = train_fin.columns.tolist()\n",
    "index_list.pop(index_list.index('target'))\n",
    "feature_label = index_list\n",
    "del index_list\n",
    "target_label = ['target']\n",
    "\n",
    "feature_data = train_fin[feature_label].values\n",
    "target_data = train_fin[target_label].values\n",
    "tscv = TimeSeriesSplit(max_train_size=None, n_splits=12)\n",
    "print(tscv)\n",
    "\n",
    "## Time Series Split 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9779131153449505\n",
      "4.3363675019371435\n",
      "3.50841950144898\n",
      "4.32609750940869\n",
      "3.8291777945802434\n",
      "3.9955059545544063\n",
      "3.931601252510613\n",
      "2.7698374576161293\n",
      "4.886394023979587\n",
      "3.4914335591048204\n",
      "4.15470467303347\n",
      "3.1864587514127667\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(feature_data):\n",
    "    X_train, X_test = feature_data[train_index], feature_data[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror', booster='gbtree', learning_rate=0.2, min_child_weight=1.5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    \n",
    "    WRMSE_true = list(map(lambda x : x[0], list(y_test)))\n",
    "    y_preds = list(np.around(np.array(list(y_preds)),0))\n",
    "    y_preds = list(map(lambda x : 0 if x <= 0 else x, y_preds))\n",
    "    WRMSE_preds = list(map(lambda x : 9 if x >= 9 else x, y_preds))\n",
    "    print(WRMSE(WRMSE_true, WRMSE_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.612793047775386\n",
      "5.240436826058216\n",
      "3.5734414344850562\n",
      "5.166631745367273\n",
      "4.417699022668659\n",
      "4.457813302807302\n",
      "3.9565111646756983\n",
      "2.73732797242002\n",
      "4.812720078411192\n",
      "3.6508338420720516\n",
      "3.7530757802515504\n",
      "3.0917429645752765\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in tscv.split(feature_data):\n",
    "    X_train, X_test = feature_data[train_index], feature_data[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective ='reg:squarederror', booster='gbtree', learning_rate=0.15, min_child_weight=0.5,\n",
    "                            max_depth=7)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_preds = model.predict(X_test)\n",
    "    \n",
    "    WRMSE_true = list(map(lambda x : x[0], list(y_test)))\n",
    "    y_preds = list(np.around(np.array(list(y_preds)),0))\n",
    "    y_preds = list(map(lambda x : 0 if x <= 0 else x, y_preds))\n",
    "    WRMSE_preds = list(map(lambda x : 9 if x >= 9 else x, y_preds))\n",
    "    print(WRMSE(WRMSE_true, WRMSE_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 3. 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>mean_Np</th>\n",
       "      <th>mean_Tp</th>\n",
       "      <th>mean_Vp</th>\n",
       "      <th>mean_B_gsm_x</th>\n",
       "      <th>mean_B_gsm_y</th>\n",
       "      <th>mean_B_gsm_z</th>\n",
       "      <th>...</th>\n",
       "      <th>std_B_gsm_y</th>\n",
       "      <th>std_B_gsm_z</th>\n",
       "      <th>std_Bt</th>\n",
       "      <th>var_Np</th>\n",
       "      <th>var_Tp</th>\n",
       "      <th>var_Vp</th>\n",
       "      <th>var_B_gsm_x</th>\n",
       "      <th>var_B_gsm_y</th>\n",
       "      <th>var_B_gsm_z</th>\n",
       "      <th>var_Bt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.195024</td>\n",
       "      <td>22761.074691</td>\n",
       "      <td>353.657878</td>\n",
       "      <td>3.089253</td>\n",
       "      <td>-1.209734</td>\n",
       "      <td>-0.107792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.364397</td>\n",
       "      <td>0.409743</td>\n",
       "      <td>0.175855</td>\n",
       "      <td>0.035094</td>\n",
       "      <td>1.333233e+07</td>\n",
       "      <td>4.868653</td>\n",
       "      <td>0.041541</td>\n",
       "      <td>0.132785</td>\n",
       "      <td>0.167890</td>\n",
       "      <td>0.030925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.556466</td>\n",
       "      <td>25284.098765</td>\n",
       "      <td>346.161446</td>\n",
       "      <td>3.257282</td>\n",
       "      <td>-1.025819</td>\n",
       "      <td>0.643181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217096</td>\n",
       "      <td>0.143583</td>\n",
       "      <td>0.051232</td>\n",
       "      <td>0.031385</td>\n",
       "      <td>8.598477e+06</td>\n",
       "      <td>6.748046</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.020616</td>\n",
       "      <td>0.002625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.429241</td>\n",
       "      <td>23017.538889</td>\n",
       "      <td>342.789659</td>\n",
       "      <td>3.080376</td>\n",
       "      <td>-1.340154</td>\n",
       "      <td>0.425454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276740</td>\n",
       "      <td>0.482064</td>\n",
       "      <td>0.085693</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>1.083478e+07</td>\n",
       "      <td>1.592721</td>\n",
       "      <td>0.027214</td>\n",
       "      <td>0.076585</td>\n",
       "      <td>0.232386</td>\n",
       "      <td>0.007343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.101499</td>\n",
       "      <td>26302.041821</td>\n",
       "      <td>338.663912</td>\n",
       "      <td>3.096789</td>\n",
       "      <td>-1.192257</td>\n",
       "      <td>0.335477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219488</td>\n",
       "      <td>0.328673</td>\n",
       "      <td>0.086919</td>\n",
       "      <td>0.017724</td>\n",
       "      <td>8.301107e+06</td>\n",
       "      <td>5.343749</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.048175</td>\n",
       "      <td>0.108026</td>\n",
       "      <td>0.007555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2.124907</td>\n",
       "      <td>37699.452581</td>\n",
       "      <td>327.827076</td>\n",
       "      <td>2.594449</td>\n",
       "      <td>-1.279867</td>\n",
       "      <td>0.991676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283880</td>\n",
       "      <td>0.271309</td>\n",
       "      <td>0.096867</td>\n",
       "      <td>0.003107</td>\n",
       "      <td>2.946165e+07</td>\n",
       "      <td>5.671399</td>\n",
       "      <td>0.143226</td>\n",
       "      <td>0.080588</td>\n",
       "      <td>0.073609</td>\n",
       "      <td>0.009383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour   mean_Np       mean_Tp     mean_Vp  mean_B_gsm_x  \\\n",
       "0  2013      1    1     0  2.195024  22761.074691  353.657878      3.089253   \n",
       "1  2013      1    1     3  2.556466  25284.098765  346.161446      3.257282   \n",
       "2  2013      1    1     6  2.429241  23017.538889  342.789659      3.080376   \n",
       "3  2013      1    1     9  2.101499  26302.041821  338.663912      3.096789   \n",
       "4  2013      1    1    12  2.124907  37699.452581  327.827076      2.594449   \n",
       "\n",
       "   mean_B_gsm_y  mean_B_gsm_z  ...  std_B_gsm_y  std_B_gsm_z    std_Bt  \\\n",
       "0     -1.209734     -0.107792  ...     0.364397     0.409743  0.175855   \n",
       "1     -1.025819      0.643181  ...     0.217096     0.143583  0.051232   \n",
       "2     -1.340154      0.425454  ...     0.276740     0.482064  0.085693   \n",
       "3     -1.192257      0.335477  ...     0.219488     0.328673  0.086919   \n",
       "4     -1.279867      0.991676  ...     0.283880     0.271309  0.096867   \n",
       "\n",
       "     var_Np        var_Tp    var_Vp  var_B_gsm_x  var_B_gsm_y  var_B_gsm_z  \\\n",
       "0  0.035094  1.333233e+07  4.868653     0.041541     0.132785     0.167890   \n",
       "1  0.031385  8.598477e+06  6.748046     0.007467     0.047131     0.020616   \n",
       "2  0.008152  1.083478e+07  1.592721     0.027214     0.076585     0.232386   \n",
       "3  0.017724  8.301107e+06  5.343749     0.012425     0.048175     0.108026   \n",
       "4  0.003107  2.946165e+07  5.671399     0.143226     0.080588     0.073609   \n",
       "\n",
       "     var_Bt  \n",
       "0  0.030925  \n",
       "1  0.002625  \n",
       "2  0.007343  \n",
       "3  0.007555  \n",
       "4  0.009383  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_testset = test_fin[feature_label].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = model.predict(final_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = list(map(lambda x : x, list(final_pred)))\n",
    "final_pred = list(np.around(np.array(list(final_pred)),0))\n",
    "final_pred = list(map(lambda x : 0 if x <= 0 else x, final_pred))\n",
    "final_pred = list(map(lambda x : 9 if x >= 9 else x, final_pred))\n",
    "\n",
    "## 설정한 결과로 변환 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_list = list()\n",
    "for i in range(int(len(final_pred)/8)):\n",
    "    final_pred_list.append(final_pred[8*i:8*(i+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_result = pd.read_excel(\"Final_prediction.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kp_0h', 'kp_3h', 'kp_6h', 'kp_9h', 'kp_12h', 'kp_15h', 'kp_18h',\n",
       "       'kp_21h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_result.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sheet = pd.DataFrame(columns=tmp_result.columns)\n",
    "final_sheet['DOY'] = tmp_result['DOY']\n",
    "final_sheet[tmp_result.columns[1:]] = pd.DataFrame(final_pred_list,columns=tmp_result.columns[1:]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 4. 결과 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sheet.to_csv('HW_Final_prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
